import torch
import torch.nn as nn
from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, LlamaAttention, Cache
from typing import Tuple, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_from_disk
from sklearn.metrics import accuracy_score, precision_score, recall_score
import pandas as pd
from tqdm import tqdm
from collections import defaultdict
from ...processing_utils import Unpack


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_linear_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    # attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class ModifiedLlamaAttention(LlamaAttention):
    def forward(
            self,
            hidden_states: torch.Tensor,
            position_embeddings: Tuple[torch.Tensor, torch.Tensor],
            attention_mask: Optional[torch.Tensor],
            past_key_value: Optional[Cache] = None,
            cache_position: Optional[torch.LongTensor] = None,
            **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_linear_attention_forward
        if self.config._attn_implementation != "eager":
            if self.config._attn_implementation == "sdpa" and kwargs.get("output_attentions", False):
                logger.warning_once(
                    "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to "
                    'eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
                )
            else:
                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights

model_path = "./models/llama-3.2-3b"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

LlamaAttention.forward = ModifiedLlamaAttention.forward
# for layer in model.model.layers:
#     layer.self_attn = ModifiedLlamaAttention(layer.self_attn, layer_idx=5)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

data_list = ['hotel', 'sst2', 'sst5', 'subj', 'agnews']

results = []
prediction_data = []

for dataset_name in data_list:
    print(f"Processing dataset: {dataset_name}")
    dataset = load_from_disk(f'./datasets/{dataset_name}')

    print(f"Fields for dataset {dataset_name}: {dataset['train'].features}")

    columns = dataset['train'].features
    label_column_name = None
    if 'label' in columns:
        label_column_name = 'label'
    elif 'labels' in columns:
        label_column_name = 'labels'

    label_column = dataset['train'].features[label_column_name]
    labels = set(dataset['train'][label_column_name])
    num_classes = len(labels)

    label_to_data = defaultdict(list)
    label_count = defaultdict(int)

    for example in tqdm(dataset['train'], desc="Selecting training data"):
        label = example[label_column_name]
        if label_count[label] < 10:
            label_to_data[label].append(example)
            label_count[label] += 1

    context = ""
    recent_labels = []

    for example in dataset['train']:
        label = example[label_column_name]

        if label_to_data[label]:
            text_field = None
            if 'text' in example:
                text_field = 'text'
            elif 'sentence' in example:
                text_field = 'sentence'
            elif 'sentence1' in example:
                text_field = 'sentence1'

            if text_field:
                if recent_labels.count(label) < 3:
                    context += f"Text: {example[text_field]}\nLabel: {example[label_column_name]}\n"
                    recent_labels.append(label)
                    label_to_data[label].pop(0)
                else:
                    continue
            else:
                raise KeyError("No valid text field found in example.")

    context_ids = tokenizer(context, return_tensors="pt").input_ids.to(device)

    if 'validation' in dataset:
        test_data = dataset['validation']
    else:
        test_data = dataset['test']

    predictions = []
    true_labels = []

    for test_example in tqdm(test_data, desc="Processing test data"):
        text_field = None
        if 'text' in test_example:
            text_field = 'text'
        elif 'sentence' in test_example:
            text_field = 'sentence'
        elif 'sentence1' in test_example:
            text_field = 'sentence1'

        if text_field:
            input_text = context + f"Text: {test_example[text_field]}\nLabel: "
        else:
            raise KeyError("No valid text field found in test example.")

        input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(device)

        with torch.no_grad():
            outputs = model(input_ids)

        logits = outputs.logits[:, -1, :]
        predicted_token = torch.argmax(logits, dim=-1)

        predicted_label = int(tokenizer.decode(predicted_token.item()))

        predictions.append(predicted_label)
        true_labels.append(test_example[label_column_name])

    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions, average='weighted')
    recall = recall_score(true_labels, predictions, average='weighted')

    print(f"Results for {dataset_name}:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")

    result = {
        'dataset': dataset_name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'true_labels': true_labels,
        'predictions': predictions,
        'context': context,
    }

    results.append(result)

    print(f"Results for {dataset_name} saved temporarily")

directory = "./results"
datasets_name = '_'.join(data_list)
filename = f"{directory}llama3.2-3b_linear_{datasets_name}_classification_results.csv"

df = pd.DataFrame(results)
df.to_csv(filename, index=False)

print(f"All results saved to classification_results.csv")
