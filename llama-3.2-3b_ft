import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments
from datasets import load_from_disk

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_path = "./models/llama-3.2-3b"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path).to(device)

dataset_name = 'sst2'
print(f"Processing dataset: {dataset_name}")
dataset = load_from_disk(f'./datasets/{dataset_name}')


def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)


tokenizer.pad_token = tokenizer.eos_token

tokenized_datasets = dataset.map(tokenize_function, batched=True)

train_dataset = tokenized_datasets["train"]

label_column = 'label'

sorted_train_dataset = sorted(train_dataset, key=itemgetter(label_column))

grouped_data = groupby(sorted_train_dataset, key=itemgetter(label_column))

top_10_data_per_label = []
for label, group in grouped_data:
    top_10_data_per_label.extend(list(group)[:10])

top_10_train_dataset = Dataset.from_dict({
    key: [item[key] for item in top_10_data_per_label] for key in top_10_data_per_label[0]
})

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=10,
    per_device_eval_batch_size=10,
    num_train_epochs=25,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=top_10_train_dataset["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
)

trainer.train()

model.save_pretrained("./fine_tuned_models")
tokenizer.save_pretrained("./fine_tuned_models")
